<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Responsible AI - Category - CastilloS</title>
        <link>http://localhost:1313/categories/responsible-ai/</link>
        <description>Responsible AI - Category - CastilloS</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 01 Jun 2023 10:49:29 -0300</lastBuildDate><atom:link href="http://localhost:1313/categories/responsible-ai/" rel="self" type="application/rss+xml" /><item>
    <title>Responsible AI Tools</title>
    <link>http://localhost:1313/responsible_ai/</link>
    <pubDate>Thu, 01 Jun 2023 10:49:29 -0300</pubDate>
    <author>CastilloCS</author>
    <guid>http://localhost:1313/responsible_ai/</guid>
    <description><![CDATA[<p>Responsible AI has become a crucial aspect of every Machine Learning project, underscoring the need for tools that promote the creation of fair and ethically sound models. In this post, we delve into some key concepts and replicate IBM&rsquo;s inFairness example solution to address bias in the &lsquo;Adult Income&rsquo; dataset. Beyond the technical configurations adopted, the intent to algorithmically address unfairness and the statistical pursuit of both overt and hidden bias in data are particularly noteworthy. These are the key insights we hope readers will take away from this post.</p>]]></description>
</item>
</channel>
</rss>
