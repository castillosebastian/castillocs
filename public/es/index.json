[{"categories":["IA"],"content":"Introducción a Modelos de Lenguaje. ","date":"2019-12-01","objectID":"/castillocs/es/llm/:0:0","tags":["LLM","Aprendizaje Computacional"],"title":"Modelos de Lenguaje","uri":"/castillocs/es/llm/"},{"categories":["IA"],"content":"Introducción Este es un apunte sobre el dominio del Procesamiento de Lenguaje Natural elaborado a partir de autores que citaremos en cada pasaje. El objetivo de este manterial es emprender una reconstrucción de este campo disciplinar desde una perspectiva epistemológica, a fin de llegar a enriquecer la reflexión que hoy se produce en torno a las tecnologías que emplean estas herramientas. Particularmente tenemos en mente aquellas implementaciones que suponen el uso de estrategias de aprendizaje formal o inteligencia artificial. ","date":"2019-12-01","objectID":"/castillocs/es/llm/:1:0","tags":["LLM","Aprendizaje Computacional"],"title":"Modelos de Lenguaje","uri":"/castillocs/es/llm/"},{"categories":["IA"],"content":"Modelos de Lenguaje La probabilidad es un aspecto central del tratamiento computacional del legunaje[1], dado el caracter ambiguo y polisémico del lenguaje, además de que sus medios de producción siempre suponen la presencia de “ruido”. La fórmula de probabilidad condicional aplicada en este tratamietno es: P(w|h) La probabilidad de una palabra dado su historia de palabras precedentes. Este computo extendido a la historia de palabras precedentes podría ser de imposible resolución dado que los contextos de las palabras pueden ser muy grandes. Por eso, apelando a la premisa de Markow sobre que la probabilidad de una palabra puede aproximarse satisfactoriamente con una observación de las ocurrencias próximas, se adoptan las ocurrencias previas como un estimador apropiado de la probabilidad de una ocurrencia dada. ¿Cómo se efectúa el cómputo de esa aproximación? La idea más intuitiva acaso sea calcular el estimador de máxima verosimilitud o MLE, se vectoriza las palabras de un corpus, se produce contando de las ocurrencias y se normalizan los valores de tal forma de que el valor asociado a la ocurrencia de cada palabra (o feature) caiga entre 0 y 1 (conforme valores de probabilidad). El ratio resultante se denomina frecuencia relativa. De esta forma se pueden trabajar n-grams donde n = 2, n = 3, n = N. Las aplicaciones normalmente usan n = 3 a n = 5 (esto último cuando el corpus es suficientemente grande.) ","date":"2019-12-01","objectID":"/castillocs/es/llm/:2:0","tags":["LLM","Aprendizaje Computacional"],"title":"Modelos de Lenguaje","uri":"/castillocs/es/llm/"},{"categories":["IA"],"content":"Restricciones del LM Las características de esta estrategia determina ciertas restricciones a su eficacia. En primer lugar, LM presenta gran dependencia de set de entrenamiento por lo que resulta poco generalizable. Eso implica que su eficacia siempre está condicionada a la similitud de generos o dominios de lenguaje. Otro restricción está dada por la subestimación de terminos cuya ocurrencia es 0 en el set de entrenamiento pero resultan frecuentes dentro del dominio de lenguaje en cuestión. Finalmente, no es extraño en muchos dominios del lenguaje la existencia de vocabularios abiertos y la ocurrencia, en tal caso, de palabras desconocidas. Estas distintas restricciones pueden tratarse de distinta forma para lograr un modelo más flexible al momento de asignar probabilidad. Implica distintas formas de suavizar la función de probabilidad asignado valores ligeramente superiores a 0. Por ejemplo partir de un conteo de frecuencia que por defecto sea 1. ","date":"2019-12-01","objectID":"/castillocs/es/llm/:3:0","tags":["LLM","Aprendizaje Computacional"],"title":"Modelos de Lenguaje","uri":"/castillocs/es/llm/"},{"categories":["IA"],"content":"Evaluación de LM La mejor forma de evaluar el desempeño de un modelo n-grams es mediante la implementación concreta y la resolución de un caso práctico. Este típo de evaluación se denomina evaluación extrínseca (end-to-end) y contrasta con la evaluación intrínseca mediante una métrica de desempeño. Esta métrica para el caso de modelos de langueja se denomina perplejidad y se colcula para cada modelo n-grams como la inversa de la probabilidad del test_set normalizado por el número de palabras (o vocabulario). Como la relación es inversa, cuando más grande sean los valores de probabilidad condicional del set de n-grams menor será la perplejidad. Maximizar la probabilidad conducirá a minimizar la perplejidad. ","date":"2019-12-01","objectID":"/castillocs/es/llm/:4:0","tags":["LLM","Aprendizaje Computacional"],"title":"Modelos de Lenguaje","uri":"/castillocs/es/llm/"},{"categories":["IA"],"content":"Práctica En esta notebook en google-colab efectuamos un ejercicio de aplicación de n-grams en Python siguiendo al curso Natural Language Processing de la especialización Advanced Machine Learning Specialization dictado a través de Coursera por National Research University Higher School of Economics, Rusia: link [1] Dan Jurafsky and James H. Martin, Speech and Language Processing (3rd ed. draft), work in progress, free access URL = https://web.stanford.edu/~jurafsky/slp3/. ","date":"2019-12-01","objectID":"/castillocs/es/llm/:5:0","tags":["LLM","Aprendizaje Computacional"],"title":"Modelos de Lenguaje","uri":"/castillocs/es/llm/"},{"categories":["Poder Judicial","Sector Público"],"content":"Este post es un recorrido de la transformación que en materia de estadística está llevando a cabo el Poder Judicial de Entre Ríos, Argentina, donde trabajo como Director. ","date":"2016-06-01","objectID":"/castillocs/es/analitics_in_judiciary/:0:0","tags":["Tableros","Cultura Analítica"],"title":"","uri":"/castillocs/es/analitics_in_judiciary/"},{"categories":["Poder Judicial","Sector Público"],"content":"2016 Creación de un Órgano Especializado El Área de Planificación Gestión y Estadística, creada en junio de 2016, emprendió el proyecto de reformar la estadística judicial de la provincia, asediada por graves dificultades, a través de un cambio profundo en las herramientas de trabajo y más ampliamente en la cultura organizacional (i.e. desde la digitalización de información y diseño de nuevos indicadores, hasta la creación de un nuevo calendario de procesos estadísticos y normativa interna).Tuvo la colaboración de magistrados y funcionarios de toda la provincia que aportaron el conocimiento sustantivo imprenscindible para la generación de métricas confiables. A pesar de las limitaciones de herramientas y personal, se logró implementar soluciones eficientes, como el uso del software de código abierto R-Statistical Computing para el procesamiento de datos. En un lapso de 10 meses, se revolucionó la estadística judicial, optimizando los tiempos de producción de informes, introduciendo nuevos indicadores y poniendo en marcha un nuevo Sistema de Estadística Pública Judicial, que se extendió a todas las ramas de la justicia provincial para junio de 2018 (mas info) ","date":"2016-06-01","objectID":"/castillocs/es/analitics_in_judiciary/:1:0","tags":["Tableros","Cultura Analítica"],"title":"","uri":"/castillocs/es/analitics_in_judiciary/"},{"categories":["Poder Judicial","Sector Público"],"content":"2019 Publicación de Tableros Públicos Luego de dos años de trabajo desafiante se habilitaron on line los Tableros de Estadística del Poder Judicial de Entre Ríos. El 17 de diciembre del 2019, la justicia provincial presentó a la sociedad una herramienta de acceso público a indicadores judiciales basada en un sistema de producción formal, sostenido íntegramente por software libre R-Statistical Computing. Tableros de Estadística Con esta presentación el equipo de trabajo que tuve la fortuna de liderar cumplió un objetivo largamente buscado en materia de datos judiciales abiertos, creando en el camino un modelo de estadística judicial único en Argentina (mas info). ","date":"2016-06-01","objectID":"/castillocs/es/analitics_in_judiciary/:2:0","tags":["Tableros","Cultura Analítica"],"title":"","uri":"/castillocs/es/analitics_in_judiciary/"},{"categories":["Poder Judicial","Sector Público"],"content":"2021 Actividad Judicial en Pandemia COVID19 Hice una breve presentación interna sobre el impacto de la Pandemia Covid-19 en la actividad judicial, con algunos detalles sobre la cantidad de procesos realizados entre 2020 y 2021, que generó mucho impacto debido a que despejó dudas respecto de la actividad de la justicia y la importancia de sus servicios (mas info).Luego de eso, realicé una presentación en el canal de televisión local sobre la coyuntura del Poder Judicial durante el aislamiento sanitario de Pandemia: Finalmente, para ese año, ya estabamos compartiando nuestra experiencia de transformación estadística con todos los Poderes Judiciales de nuestro país, reunidos en la Junta Federal de Cortes y Superiores Tribunales de Argentina (mas info). ","date":"2016-06-01","objectID":"/castillocs/es/analitics_in_judiciary/:3:0","tags":["Tableros","Cultura Analítica"],"title":"","uri":"/castillocs/es/analitics_in_judiciary/"},{"categories":["Epistemología Formal"],"content":"Una mirada elemental a la epistemología","date":"2015-03-25","objectID":"/castillocs/es/formal_epistemology/","tags":["Conocimiento"],"title":"Epistemología Formal","uri":"/castillocs/es/formal_epistemology/"},{"categories":["Epistemología Formal"],"content":"La epistemología formal es un campo interdisciplinario que reflexiona sobre el conocimiento y el aprendizaje empleando métodos formales. ","date":"2015-03-25","objectID":"/castillocs/es/formal_epistemology/:0:0","tags":["Conocimiento"],"title":"Epistemología Formal","uri":"/castillocs/es/formal_epistemology/"},{"categories":["Epistemología Formal"],"content":"Introducción En este comentario se presenta el campo de la epistemología formal como rama interdisciplinaria que reflexiona sobre el conocimiento y el aprendizaje empleando métodos formales. Estos métodos no solo incluyen herramientas que vienen de la lógica y la matemática,1 sino también -y hoy más que nunca- de la computación, particularmente de los desarrollos en el campo de la inteligencia artificial. Con todo, para avanzar en un análisis formal del conocimiento es secundario el dominio de origen de los dispositivos de análisis, basta con que asuman características formales.2 Con ello la reflexión se compromete metodológicamente con ciertos procedimientos, buscando resultados con un nivel de abstracción útil para comprender fenómenos complejos como el conocimiento y el aprendizaje. Siguiendo a Weinsberg clarifiquemos esta forma de análisis con un ejemplo. La tarea de confirmar hipótesis científicas puede abordarse desde una visión lógica de la siguiente manera: dada la hipótesis h que establece que “todos los electrónicos tienen carga negativa”, formalizada como $\\forall$x($Ex \\subset Nx$), asumiríamos ante la existencia de un individuo a con la propiedad de ser un electrón que tal individuo también tiene carga negativa. Contrastada la existencia del caso a con esas propiedades se obtendría apoyo en favor de h, es decir, dado que verificamos h en un caso particular tenemos una experiencia que abala que h se cumple para todos los casos. Así, siguiendo a Nicod (1930) y Weinsberg una generalización universal es confirmada por sus instancias positivas hasta tanto no se descubra un caso que la contradiga. Por supuesto que esta afirmación deja muchas cosas sin resolver, particularmente deja abierta la pregunta sobre cuánto peso (importancia) confirmatoria tiene una instancia particular respecto de una generalización universal. Aunque podemos evitar esta pregunta otorgando un valor absoluto a un evento confirmatorio, es inevitable pensar en el valor de una generalización en términos de los casos que la misma ha explicado satisfactoriamente. Eso dejaría a la confirmación como una magnitud. Independientemente de la resolución de estos interrogantes, el enfoque formal simplifica los elementos y relaciones bajo análisis permitiendo modelar de manera productiva problemas epistemológicos. ","date":"2015-03-25","objectID":"/castillocs/es/formal_epistemology/:1:0","tags":["Conocimiento"],"title":"Epistemología Formal","uri":"/castillocs/es/formal_epistemology/"},{"categories":["Epistemología Formal"],"content":"Aprendizaje Formal Bajo esta idea se plantean teorías acerca de cómo y bajo qué condiciones formales se genera aprendizaje a partir de observaciones. Estas teorías pueden asumir distintas formas según los objetos y problemas abordados. Por ejemplo, Schulte señala que muchos resultados en el campo del aprendizaje formal en Ciencia de la Computación están vinculados a la noción de Valiant y Vapnik sobre aprendizaje de generalizaciones aproximadamente correctas desde una perspectiva de probabilidad.3 La aproximación a la corrección se vincula estrechamente a la noción de éxito empírico introducida por Gilbert Harmann, y retomada por Valiant en su reflexión sobre los problemas de la inducción (Valiant, 2013, Ch. 5). En cualquier caso, el aprendizaje formal remite generalmente a un análisis epistemológico contextualizado donde se recorta un problema empírico puntual y un resultado esperado en términos de aprendizaje. Por esto Schulte señala que la mayorías de las teorías de aprendizaje [formal] examinan qué estrategias de investigación resultan más confiables y eficientes para generar creencias [conocimiento] acerca del mundo. ","date":"2015-03-25","objectID":"/castillocs/es/formal_epistemology/:2:0","tags":["Conocimiento"],"title":"Epistemología Formal","uri":"/castillocs/es/formal_epistemology/"},{"categories":["Epistemología Formal"],"content":"Aprendizaje Profundo El Aprendizaje Profundo (AP) es una técnica mediante la cual un agente adquiere la capacidad de ‘aprender’ de la experiencia almacenada en forma de dato. Esta técnica forma parte del campo de la Inteligencia Artificial que, en líneas generales, procura crear agentes capaces de realizar tareas que suponen habilidades intelectuales complejas, tareas como reconocer imágenes, procesar y producir lenguaje, identificar patrones, entre otras. En el centro del AP se encuentra el viejo problema epistemológico de generar ‘buenas representaciones’ de objetos de conocimiento; problema que el AP resuelve a través de representar el mundo como una estructura jerárquica de conceptos anidados, donde cada concepto se define en relación a conceptos más simples, y donde las representaciones más abstractas se computan a partir de otras menos abstractas (Goodfellow y ot. 2016:8). Por esa razón una de las tareas importantes del AP es la transformación algorítmica de conceptos de unidades simples en unidades complejas. Para generar representaciones de objetos, y a diferencia de otras técnicas de aprendizaje formal, el AP posee la capacidad de identificar características definitorias de ciertos objetos (features) y generar modelos (representaciones) a partir de ellas. Esta capacidad de generar modelos es autónoma en un sentido estricto: el AP no posee modelos previos de sus objetos, los construye mediante funciones matemáticas. Para establecer una analogía con el ser humano podríamos pensar en que hasta no hace mucho solamente una persona podía mirar 10 mil fotos de sillas y crear un modelo para reconocer si una nueva foto (la 10.001) es una silla o no. Ahora un agente que aplica AP puede hacer lo mismo, de forma asombrosamente rápida y probadamente más efectiva. El ‘aprendizaje de representaciones’ es un aspecto definitorio del AP, e implica una tarea simultanea de identificar características distintivas de objetos aislándolas de factores de variación particular siempre presentes en la experiencia. Para esto, el AP genera sus representaciones complejas (el modelo de silla) componiéndolas a partir de representaciones simples. La noción de ‘aprendizaje profundo’ proviene del hecho que esta composición adquiere la forma de procesamiento en niveles o capas de información.4 Weisberg, Jonathan, “Formal Epistemology”, The Stanford Encyclopedia of Philosophy (Winter 2017 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/win2017/entries/formal-epistemology/. ↩︎ Características que -por ahora- podemos relacionar con la idea de un lenguaje explícito (semántica y sintácticamente) con reglas definidas de producción e interpretación. ↩︎ Schulte, Oliver, “Formal Learning Theory”, The Stanford Encyclopedia of Philosophy (Spring 2018 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/spr2018/entries/learning-formal/. ↩︎ Chollet-Allaire, Deep Learning with R, 2017. ↩︎ ","date":"2015-03-25","objectID":"/castillocs/es/formal_epistemology/:3:0","tags":["Conocimiento"],"title":"Epistemología Formal","uri":"/castillocs/es/formal_epistemology/"},{"categories":null,"content":"Las aplicaciones que he construido","date":"2019-08-02","objectID":"/castillocs/es/aplications/","tags":null,"title":"Las aplicaciones que he construido","uri":"/castillocs/es/aplications/"},{"categories":null,"content":"Tablero de Justicia El Tablero de Justicia es el servicio en línea del Superior Tribunal de Justicia de Entre Ríos, Argentina, que permite acceder a datos primarios y estadísticas de justicia. Está basado en el software libre R y Shiny y sigue los estándares del movimiento de datos abiertos. La parte central del sistema, los scripts estadísticos, están accesibles en operaciones, procesamiento, organización e investigación. Datos de Justicia ","date":"2019-08-02","objectID":"/castillocs/es/aplications/:1:0","tags":null,"title":"Las aplicaciones que he construido","uri":"/castillocs/es/aplications/"},{"categories":null,"content":"Características  Optimizado para el acceso público y la reproducibilidad de las estadísticas.  Cumplimiento de los estándares nacionales e internacionales sobre información pública y datos sensibles.  Acceso completo del usuario a algoritmos y scripts. ","date":"2019-08-02","objectID":"/castillocs/es/aplications/:1:1","tags":null,"title":"Las aplicaciones que he construido","uri":"/castillocs/es/aplications/"},{"categories":null,"content":"Aeropuertos en el Mundo La aplicación Aeropuertos en el Mundo es un proyecto-tarea desarrollado para el seminario de Visualizaciones de la Universidad Tecnológica Nacional, que incorpora una base de datos de aeropuertos alrededor del mundo. Utilizando Python y Dash, presenta una interfaz interactiva y amigable para que los usuarios exploren los datos. Está hosteado en Heroku. Aeropuertos ","date":"2019-08-02","objectID":"/castillocs/es/aplications/:2:0","tags":null,"title":"Las aplicaciones que he construido","uri":"/castillocs/es/aplications/"},{"categories":null,"content":"Características  Base de Datos Mundial: La aplicación proporciona información (no actualizada) sobre los aeropuertos del mundo. Los usuarios pueden acceder a información específica como ubicación, tamaño del aeropuerto y más.  Interactivo: la aplicación es un mapa interactivo que permite visualizar la distribución geográfica de los aeropuertos en el mundo. Los usuarios pueden acercar y alejar para ver aeropuertos en regiones o países específicos, facilitando la comprensión de la infraestructura aeroportuaria global.\" ","date":"2019-08-02","objectID":"/castillocs/es/aplications/:2:1","tags":null,"title":"Las aplicaciones que he construido","uri":"/castillocs/es/aplications/"}]