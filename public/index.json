[{"categories":["Dashboards","Data driven culture"],"content":"This post chronicles the transformative journey that the Judiciary of Entre Ríos, Argentina, where I serve as Director, is undertaking in the realm of statistics. ","date":"2019-12-01","objectID":"/castillocs/analitics_in_judiciary/:0:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/castillocs/analitics_in_judiciary/"},{"categories":["Dashboards","Data driven culture"],"content":"2016 Creation of a Specialized Body The Area of Planning, Management and Statistics, created in June 2016, undertook the project of reforming the judicial statistics of the province, beset by serious difficulties, through a profound change in working tools and more broadly in organizational culture (i.e. from the digitization of information and the design of new indicators, to the creation of a new calendar of statistical processes and internal regulations). It had the collaboration of magistrates and officials from all over the province who provided the substantive knowledge necessary for the generation of reliable judicial metrics. Despite limitations in tools and staff, efficient solutions were implemented, such as the use of the open-source software R-Statistical Computing for data processing. In a span of 10 months, judicial statistics were revolutionized, optimizing report production times, introducing new indicators, and launching a new Public Judicial Statistics System, which extended to all branches of provincial justice by June 2018 (more info). ","date":"2019-12-01","objectID":"/castillocs/analitics_in_judiciary/:1:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/castillocs/analitics_in_judiciary/"},{"categories":["Dashboards","Data driven culture"],"content":"2019 Public Dashboards After two challenging years of work, the Statistics Dashboards of the Judicial Power of Entre Ríos were made available online. On December 17, 2019, the provincial justice system presented to society a public access tool to judicial indicators based on a formal production system, entirely supported by the open-source software R-Statistical Computing. Statistics Dashboards With this presentation, the team I was fortunate to lead fulfilled a long-sought goal in terms of open judicial data, creating along the way a unique model of judicial statistics in Argentina (more info). ","date":"2019-12-01","objectID":"/castillocs/analitics_in_judiciary/:2:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/castillocs/analitics_in_judiciary/"},{"categories":["Dashboards","Data driven culture"],"content":"2021 Judicial Activity in COVID19 Pandemic I made a brief internal presentation on the impact of the Covid-19 Pandemic on judicial activity, with some details about the number of processes carried out between 2020 and 2021, which had a significant impact as it dispelled doubts about the activity of justice and the importance of its services (more info). After that, I gave a presentation on local television about the situation of the Judiciary during the sanitary isolation of the Pandemic: Finally, by the end of that year, we were already sharing our statistical transformation experience with all Judicial Powers in our country, gathered in the Federal Council of Courts and Superior Courts of Argentina (more info). ","date":"2019-12-01","objectID":"/castillocs/analitics_in_judiciary/:3:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/castillocs/analitics_in_judiciary/"},{"categories":["AI"],"content":"Introduction to Lenguage Models. ","date":"2019-01-01","objectID":"/castillocs/llm/:0:0","tags":["LLM","Machine Learning"],"title":"Language Models","uri":"/castillocs/llm/"},{"categories":["AI"],"content":"Introduction This is a note about the domain of Natural Language Processing compiled from authors that we will cite in each passage. The objective of this material is to undertake a reconstruction of this disciplinary field from an epistemological perspective, in order to enrich the reflection that is currently being produced around the technologies that use these tools. We particularly have in mind those implementations that involve the use of strategies of formal learning or artificial intelligence. ","date":"2019-01-01","objectID":"/castillocs/llm/:1:0","tags":["LLM","Machine Learning"],"title":"Language Models","uri":"/castillocs/llm/"},{"categories":["AI"],"content":"Language Models Probability is a central aspect of computational language processing[1], given the ambiguous and polysemic nature of language, in addition to the fact that its means of production always suppose the presence of “noise”. The formula of conditional probability applied in this treatment is: P(w|h) The probability of a word given its history of preceding words. This computation extended to the history of preceding words could be of impossible resolution since the contexts of the words can be very large. Therefore, appealing to Markov’s premise that the probability of a word can be satisfactorily approximated with an observation of the nearby occurrences, previous occurrences are adopted as an appropriate estimator of the probability of a given occurrence. How is the computation of this approximation performed? The most intuitive idea may be to calculate the maximum likelihood estimator or MLE, the words of a corpus are vectorized, the occurrences are counted and the values are normalized in such a way that the value associated with the occurrence of each word (or feature) falls between 0 and 1 (according to probability values). The resulting ratio is called relative frequency. In this way, n-grams can be worked where n = 2, n = 3, n = N. Applications normally use n = 3 to n = 5 (the latter when the corpus is large enough.) ","date":"2019-01-01","objectID":"/castillocs/llm/:2:0","tags":["LLM","Machine Learning"],"title":"Language Models","uri":"/castillocs/llm/"},{"categories":["AI"],"content":"LM Limitations The characteristics of this strategy determine certain restrictions on its effectiveness. First of all, LM has a high dependence on the training set, so it is not very generalizable. This implies that its effectiveness is always conditioned by the similarity of genres or language domains. Another restriction is given by the underestimation of terms whose occurrence is 0 in the training set but are frequent within the language domain in question. Finally, it is not uncommon in many language domains the existence of open vocabularies and the occurrence, in such a case, of unknown words. These different restrictions can be dealt with in different ways to achieve a more flexible model when assigning probability. It implies different ways of smoothing the probability function assigning slightly higher values to 0. For example, starting from a frequency count that is by default 1. ","date":"2019-01-01","objectID":"/castillocs/llm/:3:0","tags":["LLM","Machine Learning"],"title":"Language Models","uri":"/castillocs/llm/"},{"categories":["AI"],"content":"LM Evaluation The best way to evaluate the performance of an n-grams model is through concrete implementation and the resolution of a practical case. This type of evaluation is called extrinsic evaluation (end-to-end) and contrasts with intrinsic evaluation using a performance metric. This metric for language models is called perplexity and is calculated for each n-grams model as the inverse of the probability of the test_set normalized by the number of words (or vocabulary). As the relationship is inverse, the larger the conditional probability values of the n-grams set, the lower the perplexity. Maximizing the probability will lead to minimizing perplexity. ","date":"2019-01-01","objectID":"/castillocs/llm/:4:0","tags":["LLM","Machine Learning"],"title":"Language Models","uri":"/castillocs/llm/"},{"categories":["AI"],"content":"Practice In this Google Colab notebook, we perform an exercise of applying n-grams in Python following the Natural Language Processing course from the Advanced Machine Learning Specialization dictated through Coursera by National Research University Higher School of Economics, Russia: [link](https://colab.research.google.com/drive/15mPt4LS1la1 ","date":"2019-01-01","objectID":"/castillocs/llm/:5:0","tags":["LLM","Machine Learning"],"title":"Language Models","uri":"/castillocs/llm/"},{"categories":["Formal Epistemology"],"content":"Formal epistemology is an interdisciplinary field that reflects on knowledge and learning using formal methods. ","date":"2015-03-25","objectID":"/castillocs/formal_epistemology/:0:0","tags":["Knowledge"],"title":"Formal Epistemology","uri":"/castillocs/formal_epistemology/"},{"categories":["Formal Epistemology"],"content":"Introduction In this comment, the field of formal epistemology is introduced as an interdisciplinary branch that reflects on knowledge and learning using formal methods. These methods not only include tools that come from logic and mathematics,1 but also - and today more than ever - from computing, particularly developments in the field of artificial intelligence. Nevertheless, to advance in a formal analysis of knowledge, it is secondary where the analysis devices originate from, as long as they assume formal characteristics.2 This commits the reflection methodologically to certain procedures, seeking results with a level of abstraction useful for understanding complex phenomena such as knowledge and learning. Following Weinsberg, let’s clarify this form of analysis with an example. The task of confirming scientific hypotheses can be approached from a logical point of view as follows: given the hypothesis h stating that “all electrons have a negative charge”, formalized as $\\forall$x($Ex \\subset Nx$), we would assume, in the presence of an individual a with the property of being an electron, that such an individual also has a negative charge. The existence of case a with these properties would provide support in favor of h, that is, given that we verify h in a particular case, we have an experience that supports h being fulfilled in all cases. Thus, following Nicod (1930) and Weinsberg, a universal generalization is confirmed by its positive instances until a case is found that contradicts it. Of course, this statement leaves many things unresolved, particularly it leaves the question open as to how much weight (importance) a particular instance has in confirming a universal generalization. Although we can avoid this question by giving an absolute value to a confirmatory event, it is inevitable to think about the value of a generalization in terms of the cases it has satisfactorily explained. This would leave confirmation as a magnitude. Regardless of the resolution of these questions, the formal approach simplifies the elements and relationships under analysis, allowing epistemological problems to be productively modeled.\" ","date":"2015-03-25","objectID":"/castillocs/formal_epistemology/:1:0","tags":["Knowledge"],"title":"Formal Epistemology","uri":"/castillocs/formal_epistemology/"},{"categories":["Formal Epistemology"],"content":"Formal Learning Under this idea, theories are proposed about how and under what formal conditions learning is generated from observations. These theories can take different forms depending on the objects and problems addressed. For example, Schulte notes that many results in the field of formal learning in Computer Science are linked to the notion of Valiant and Vapnik on learning of approximately correct generalizations from a probability perspective.3 The approach to correction is closely linked to the notion of empirical success introduced by Gilbert Harmann, and revisited by Valiant in his reflection on the problems of induction (Valiant, 2013, Ch. 5). In any case, formal learning generally refers to a contextualized epistemological analysis where a specific empirical problem and an expected outcome in terms of learning are highlighted. This is why Schulte points out that the majority of [formal] learning theories examine which research strategies are most reliable and efficient in generating beliefs [knowledge] about the world. ","date":"2015-03-25","objectID":"/castillocs/formal_epistemology/:2:0","tags":["Knowledge"],"title":"Formal Epistemology","uri":"/castillocs/formal_epistemology/"},{"categories":["Formal Epistemology"],"content":"Deep Learning Deep Learning (DL) is a technique by which an agent acquires the ability to ’learn’ from experience stored in the form of data. This technique is part of the field of Artificial Intelligence which, in general terms, seeks to create agents capable of performing tasks that involve complex intellectual skills, tasks such as recognizing images, processing and producing language, identifying patterns, among others. At the heart of DL is the old epistemological problem of generating ‘good representations’ of knowledge objects; a problem that DL solves by representing the world as a hierarchical structure of nested concepts, where each concept is defined in relation to simpler concepts, and where the more abstract representations are computed from less abstract ones (Goodfellow et al. 2016:8). For this reason, one of the important tasks of DL is the algorithmic transformation of concepts from simple units into complex units. To generate representations of objects, and unlike other formal learning techniques, DL has the ability to identify defining characteristics of certain objects (features) and generate models (representations) from them. This ability to generate models is autonomous in a strict sense: DL does not have previous models of its objects, it constructs them using mathematical functions. To establish an analogy with humans, we might think that until not long ago, only a person could look at 10,000 photos of chairs and create a model to recognize whether a new photo (the 10,001st) is a chair or not. Now an agent that applies DL can do the same, in an amazingly fast and provenly more effective way. The ’learning of representations’ is a defining aspect of DL, and implies a simultaneous task of identifying distinctive features of objects by isolating them from particular variation factors always present in experience. For this, DL generates its complex representations (the chair model) by composing them from simple representations. The notion of ‘deep learning’ comes from the fact that this composition takes the form of processing at levels or layers of information.4 Weisberg, Jonathan, “Formal Epistemology”, The Stanford Encyclopedia of Philosophy (Winter 2017 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/win2017/entries/formal-epistemology/. ↩︎ Characteristics that -for now- we can relate to the idea of an explicit language (semantically and syntactically) with defined rules of production and interpretation. ↩︎ Schulte, Oliver, “Formal Learning Theory”, The Stanford Encyclopedia of Philosophy (Spring 2018 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/spr2018/entries/learning-formal/. ↩︎ Chollet-Allaire, Deep Learning with R, 2017. ↩︎ ","date":"2015-03-25","objectID":"/castillocs/formal_epistemology/:3:0","tags":["Knowledge"],"title":"Formal Epistemology","uri":"/castillocs/formal_epistemology/"},{"categories":null,"content":"The aplications that I've build","date":"2019-08-02","objectID":"/castillocs/aplications/","tags":null,"title":"The aplications that I've build","uri":"/castillocs/aplications/"},{"categories":null,"content":"Justice Dashboard The Justice Dashboard is the online service of the Supreme Court of Entre Ríos, Argentina, to access primary data and justice statistics. It is based on free software R and Shiny and follows the standards of the open data movement. The central part of the system, the statistical scripts, are accessible in operations, proccesing, organization and research. Justice Data ","date":"2019-08-02","objectID":"/castillocs/aplications/:1:0","tags":null,"title":"The aplications that I've build","uri":"/castillocs/aplications/"},{"categories":null,"content":"Features  Optimized for public access and reproducibility of statistics.  Compliance with national and international standards on public information and sensitive data.  Full user access of algorithms and scripts. ","date":"2019-08-02","objectID":"/castillocs/aplications/:1:1","tags":null,"title":"The aplications that I've build","uri":"/castillocs/aplications/"},{"categories":null,"content":"Airports Worldwide The Airports Worldwide app is a pet-project developed for the Universidad Tecnológica Nacional, Visualizations seminar, embodying a database of airports around the globe. Build with Python and Dash, it presents an interactive and user-friendly interface for users to explore the data. It is deploy on Heroku. Airports ","date":"2019-08-02","objectID":"/castillocs/aplications/:2:0","tags":null,"title":"The aplications that I've build","uri":"/castillocs/aplications/"},{"categories":null,"content":"Features  Worlwide Database: The application provides information about airports worldwide. Users can access specifics information as location, airport size and more.  Interactive: the application enabling users to visualize the geographical distribution of airports across the world. Users can zoom in and out to view airports in specific regions or countries, making it easy to understand global airport infrastructure. ","date":"2019-08-02","objectID":"/castillocs/aplications/:2:1","tags":null,"title":"The aplications that I've build","uri":"/castillocs/aplications/"}]