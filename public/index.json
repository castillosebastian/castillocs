[{"categories":["Generative Models"],"content":"Generative models are a class of statistical models that aim to learn the underlying data distribution from a given dataset. These models provide a way to generate new samples that are statistically similar to the training data. They have gained substantial attention in various domains, such as image generation, speech synthesis, and even drug discovery. ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:0","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Generative Models"],"content":"Generative Model Generative models are a class of statistical models that aim to learn the underlying data distribution. Given a dataset of observed samples, one starts by selecting a distributional model parameterized by $(\\theta)$. The objective is to estimate $(\\theta)$ such that it aligns optimally with the observed samples.The anticipation is that it can also generalize to samples outside the training set. The optimal distribution is hence the one that maximizes the likelihood of producing the observed data, giving lower probabilities to infrequent observations and higher probabilities to the more common ones (the principle underlying this assumption is: ’the world is a boring place’ Bhiksha Raj). ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:1","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Generative Models"],"content":"The Challenge of Maximum Likelihood Estimates (MLE) for Unseen Observations When training generative models, a natural objective is to optimize the model parameters such that the likelihood of the observed data under the model is maximized. This method is known as Maximum Likelihood Estimation (MLE). In mathematical terms, given observed data $X$, the MLE seeks parameters $\\theta$ that maximize: $p_\\theta(X)$ However, for many generative models, especially those that involve latent or unobserved variables, the likelihood term involves summing or integrating over all possible configurations of these latent variables. Mathematically, this turns into: $p_\\theta(X) = \\sum_{Z} p_\\theta(X,Z)$ or $p_\\theta(X) = \\int p_\\theta(X,Z) dZ$ Computing the log-likelihood, which is often used for numerical stability and optimization ease, leads to a log of summations (for discrete latent variables) or a log of integrals (for continuous latent variables): $log p_\\theta(X) = \\log \\sum_{Z} p_\\theta(X,Z)$ or $log p_\\theta(X) = \\log \\int p_\\theta(X,Z) dZ$ These expressions are typically intractable to optimize directly due to the presence of the log-sum or log-integral operations. Where do these formulas come frome? They come forom marginalization in the context of joint probability. ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:2","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Generative Models"],"content":"Marginalization in the Context of Joint Probability When discussing the computation of the joint probability for observed and missing data, the term “marginalizing” refers to summing or integrating over all possible outcomes of the missing data. This process provides a probability distribution based solely on the observed data. For example, let’s assume: $X$ is the observed data $Z$ is the missing data The joint probability for both is represented as $p(X,Z)$ If your primary interest lies in the distribution of $X$ and you wish to eliminate the dependence on $Z$, you’ll need to carry out marginalization for $Z$. For discrete variables, the marginalization involves the logarithm of summation: $$ \\log \\left( \\sum_{z} p(X,Z=z) \\right),$$ for continuous variables, it pertains to integration: $$ \\int p(X,Z) dZ $$ However, it’s essential to note that these functions includes the log of a sum, which defies direct optimization. Can we get an approximation to this that is more tractable (without a summation or integral within the log)? ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:3","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Generative Models"],"content":"Overcoming the Challenge with Expectation Maximization (EM) To address the optimization challenge in MLE with latent variables, the Expectation Maximization (EM) algorithm is employed. The EM algorithm offers a systematic approach to iteratively estimate both the model parameters and the latent variables. The algorithm involves two main steps: E-step (Expectation step): involves computing the expected value of the complete-data log-likelihood with respect to the posterior distribution of the latent variables given the observed data. M-step (Maximization step): Update the model parameters to maximize this expected log-likelihood from the E-step. By alternating between these two steps, EM ensures that the likelihood increases with each iteration until convergence, thus providing a practical method to fit generative models with latent variables. For E-step the Variational Lower Bound is used. Commonly referred to as the Empirical Lower BOund (ELBO), is a central concept in variational inference. This method is used to approximate complex distributions (typically posterior distributions) with simpler, more tractable ones. The ELBO is an auxiliary function that models MLE by iteratibly maximize a ’lower bound’ function Variational inference is set up as an optimization problem where the objective is to find a distribution $q$ from a simpler family of distributions that is close to the target distribution $p$. The closeness is measured using the Kullback-Leibler (KL) divergence. The ELBO serves as the objective function to be maximized in this optimization. The ELBO can be derived from the logarithm of the marginal likelihood of the observed data. Given: $p(\\mathbf{x})$ is the marginal likelihood of the observed data $\\mathbf{x}$, $q(\\mathbf{z})$ is the variational distribution over the latent variables $\\mathbf{z}$, $p(\\mathbf{z} | \\mathbf{x})$ is the true posterior distribution of $\\mathbf{z}$. The ELBO is given by: ${ELBO}(q) = \\mathbb{E}_q[\\log p(\\mathbf{x}, \\mathbf{z})] - \\mathbb{E}_q[\\log q(\\mathbf{z})]$ Where: $\\mathbb{E}_q$ denotes the expectation with respect to the distribution $q$. The first term on the right-hand side measures how well the joint distribution $p(\\mathbf{x}, \\mathbf{z})$ is modeled by $q$. -The second term is the entropy of $q$, which acts as a regularizer. Maximizing the ELBO is equivalent to minimizing the KL divergence between the variational distribution $q$ and the true posterior $p(\\mathbf{z} | \\mathbf{x})$. Intuitively, by maximizing the ELBO, you’re trying to find a balance between a distribution $q$ that closely matches the target distribution $p$ (as measured by the joint likelihood) and one that maintains uncertainty (as measured by its entropy). In SUMMARY, generative models offer a powerful approach to understanding and generating data. The challenges posed by the MLE in the presence of latent variables are effectively addressed by the EM algorithm, making it a cornerstone method in the training of many generative models. ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:4","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Generative Models"],"content":"Variational Autoencoders (VAEs) Variational Autoencoders are a specific type of generative model that brings together ideas from deep learning and Bayesian inference. VAEs are especially known for their application in generating new, similar data to the input data (like images or texts) and for their ability to learn latent representations of data. 1. Generative Models and Latent Variables In generative modeling, our goal is to learn a model of the probability distribution from which a dataset is drawn. The model can then be used to generate new samples. A VAE makes a specific assumption that there exist some latent variables (or hidden variables) that when transformed give rise to the observed data. Let $x$ be the observed data and $z$ be the latent variables. The generative story can be seen as: Draw $z$ from a prior distribution, $p(z)$. Draw $x$ from a conditional distribution, $p(x|z)$. 2. Problem of Direct Inference As discussed previously, direct inference for the posterior distribution $p(z|x)$ (i.e., the probability of the latent variables given the observed data) can be computationally challenging, especially when dealing with high-dimensional data or complex models. This is because: $$ p(z|x) = \\frac{p(x|z) p(z)}{p(x)} $$ Here, $p(x)$ is the evidence (or marginal likelihood) which is calculated as: $$ p(x) = \\int p(x|z) p(z) dz $$ This integral is intractable for most interesting models. 3. Variational Inference and ELBO To sidestep the intractability of the posterior, VAEs employ variational inference. Instead of computing the posterior directly, we introduce a parametric approximate posterior distribution, $q_{\\phi}(z|x)$, with its own parameters $\\phi$. The goal now shifts to making this approximation as close as possible to the true posterior. This is done by minimizing the Kullback-Leibler divergence between the approximate and true posterior. The optimization objective, known as the Evidence Lower BOund (ELBO), can be written as: $$ \\text{ELBO}(\\phi) = \\mathbb{E}{q{\\phi}(z|x)}[\\log p(x|z)] - \\text{KL}(q_{\\phi}(z|x) || p(z)) $$ Where KL represents the Kullback-Leibler divergence. 4. Neural Networks and Autoencoding Structure In VAEs, neural networks are employed to parameterize the complex functions. Specifically: Encoder Network: This maps the observed data, $x$, to the parameters of the approximate posterior, $q_{\\phi}(z|x)$. Decoder Network: Given samples of $z$ drawn from $q_{\\phi}(z|x)$, this maps back to the data space, outputting parameters for the data likelihood, $p_{\\theta}(x|z)$. The “autoencoder” terminology comes from the encoder-decoder structure where the model is trained to reconstruct its input data. 5. Training a VAE The training process involves: Forward pass: Input data is passed through the encoder to obtain parameters of $q_{\\phi}(z|x)$. Sampling: Latent variables $z$ are sampled from $q_{\\phi}(z|x)$ using the reparameterization trick for backpropagation. Reconstruction: The sampled $z$ values are passed through the decoder to obtain the data likelihood parameters, $p_{\\theta}(x|z)$. Loss Computation: Two terms are considered - reconstruction loss (how well the VAE reconstructs the data) and the KL divergence between $q_{\\phi}(z|x)$ and $p(z)$. Backpropagation and Optimization: The model parameters $\\phi$ and $\\theta$ are updated to maximize the ELBO. By the end of the training, you’ll have a model that can generate new samples resembling your input data by simply sampling from the latent space and decoding the samples. VAEs are a powerful tools, that stay in the intersection of deep learning and probabilistic modeling, and they have a plethora of applications, especially in unsupervised learning tasks. ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:5","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Generative Models"],"content":"Variational Encoders with Pytorch Let create a basic implementation of a Variational Autoencoder (VAE) using PyTorch. The VAE will be designed to work on simple image data, such as the MNIST dataset. import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision from torchvision import datasets, transforms # Define the VAE architecture class VAE(nn.Module): def __init__(self, input_dim, hidden_dim, latent_dim): super(VAE, self).__init__() # Encoder self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc21 = nn.Linear(hidden_dim, latent_dim) # mu self.fc22 = nn.Linear(hidden_dim, latent_dim) # logvar # Decoder self.fc3 = nn.Linear(latent_dim, hidden_dim) self.fc4 = nn.Linear(hidden_dim, input_dim) self.latent_dim = latent_dim # Add this line def encode(self, x): h1 = F.relu(self.fc1(x)) return self.fc21(h1), self.fc22(h1) def reparameterize(self, mu, logvar): std = torch.exp(0.5*logvar) eps = torch.randn_like(std) return mu + eps*std def decode(self, z): h3 = F.relu(self.fc3(z)) return torch.sigmoid(self.fc4(h3)) def forward(self, x): mu, logvar = self.encode(x.view(-1, 784)) z = self.reparameterize(mu, logvar) return self.decode(z), mu, logvar # Loss function: Reconstruction + KL Divergence Losses summed over all elements and batch def loss_function(recon_x, x, mu, logvar): BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum') # KLD = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2) KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) return BCE + KLD def train(epoch): model.train() train_loss = 0 for batch_idx, (data, _) in enumerate(train_loader): optimizer.zero_grad() recon_batch, mu, logvar = model(data) loss = loss_function(recon_batch, data, mu, logvar) loss.backward() train_loss += loss.item() optimizer.step() if batch_idx % 100 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item() / len(data))) print('====\u003e Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset))) def test(): model.eval() test_loss = 0 with torch.no_grad(): for i, (data, _) in enumerate(test_loader): recon_batch, mu, logvar = model(data) test_loss += loss_function(recon_batch, data, mu, logvar).item() if i == 0: n = min(data.size(0), 8) comparison = torch.cat([data[:n], recon_batch.view(batch_size, 1, 28, 28)[:n]]) torchvision.utils.save_image(comparison.cpu(), 'reconstruction_' + str(epoch) + '.png', nrow=n) test_loss /= len(test_loader.dataset) print('====\u003e Test set loss: {:.4f}'.format(test_loss)) transform = transforms.Compose([transforms.ToTensor()]) train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True) test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True) batch_size = 128 train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) model = VAE(input_dim=784, hidden_dim=400, latent_dim=20) optimizer = optim.Adam(model.parameters(), lr=1e-3) # Run the training loop epochs = 10 for epoch in range(1, epochs + 1): train(epoch) test() ","date":"2023-09-01","objectID":"/variational_autoencoders/:0:6","tags":["expectation maximization","variational autoencoders","deep neural networks"],"title":"Generative models: variational autoencoders","uri":"/variational_autoencoders/"},{"categories":["Responsible AI"],"content":"Responsible AI has become a crucial aspect of every Machine Learning project, underscoring the need for tools that promote the creation of fair and ethically sound models. In this post, we delve into some key concepts and replicate IBM’s inFairness example solution to address bias in the ‘Adult Income’ dataset. Beyond the technical configurations adopted, the intent to algorithmically address unfairness and the statistical pursuit of both overt and hidden bias in data are particularly noteworthy. These are the key insights we hope readers will take away from this post. ","date":"2023-06-01","objectID":"/responsible_ai/:0:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Unfearness and Responsible AI The pervasive application of machine learning in many sensitive environments to make important and life-changing decisions, has heightened concerns about the fairness and ethical impact of these technologies. More importantly, experiments that unveil biases and disparities inherent in these implementations (Mehrabi, 2022) have dismantled the idea of algorithmic ’neutrality’, emphasizes the critical need for alignment with laws and values pertaining to human dignity. In this context, the concept of responsible AI has emerged as a crucial component of every AI project, underscoring the need for procedures that can facilitate the creation of safe, fair, and ethically grounded tools (Richardson, 2021). ","date":"2023-06-01","objectID":"/responsible_ai/:1:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Fair AI tools We can view the concept of fair AI tools as pointing to software that is free from unintentional algorithmic bias. Fairness, as defined by Mehrabi et al. (2021), is the absence of any prejudice or favoritism toward an individual or a group based on their inherent or acquired characteristics. What is the difference between individual and group fairness? ","date":"2023-06-01","objectID":"/responsible_ai/:2:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Individual an Group Fairness A brief overview of the concepts of individual and group fairness as defined by Dwork et al. in their 2011 paper “Fairness Through Awareness.” Individual Fairness: According to Dwork et al., individual fairness is the principle that similar individuals should be treated similarly. This means that an algorithm is individually fair if it gives similar outputs for similar inputs. The definition of “similarity” can vary depending on the context, but it is generally defined in terms of a metric or distance function over the input space. The formal definition of individual fairness is as follows: Given a metric space (X, d) and a function f: X → Y, we say that f is Lipschitz if for all x, x’ ∈ X, d_Y(f(x), f(x’)) ≤ d_X(x, x’). In the context of fairness, this means that the difference in the outputs of the function (i.e., the decisions made by the algorithm) should not be greater than the difference in the inputs (i.e., the individuals being considered). Group Fairness: Group fairness, on the other hand, is the principle that different groups should be treated similarly on average. This means that an algorithm is group-fair if it gives similar outcomes for different groups, even if the individuals within those groups are not similar. The formal definition of group fairness can vary depending on the specific notion of fairness being considered (e.g., demographic parity, equal opportunity, etc.). However, a common definition is that the decision rates (i.e., the proportion of positive outcomes) should be equal for different groups. For example, if we denote by P(Y=1|A=a) the probability of a positive outcome given group membership a, then demographic parity requires that P(Y=1|A=a) = P(Y=1|A=a’) for all groups a, a'. To achieve fairness-related goals, we can approach them through both product development and the implementation of specific procedures: Products: refers to AI software that is designed and developed with fairness in mind. This could involve algorithms that mitigate bias or tools that promote transparency in AI decision-making. Regarding solutions, Richardson asserts that fair AI consists of strategies to combat algorithmic bias. These often include top-tier solutions drawn from research in explainability, transparency, interpretability, and accountability (Richardson, 2021). Procedures: refers to standardized activities or practices that ensure fairness. This could include ethical guidelines for AI development, rigorous testing for bias in AI systems, and policies for responsible AI use. It’s important to note that the specifics of these ‘products’ and ‘procedures’ can vary significantly depending on the context, the specific AI application, and the definition of ‘fairness’ in use. Fairness is a time-bound and context-dependent moral concept, so tools designed to ensure it must adapt to evolving standards. This means they must be flexible to changes in societal values and expectations over time. That is why the pursuit of fair AI tools is a continuous and context-specific endeavor, which rules out the possibility of universally applicable or one-size-fits-all solutions. As stated in Fairlearn project (Microsoft): ‘because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible.’ ","date":"2023-06-01","objectID":"/responsible_ai/:3:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"A fair tool: InFairness Now, let’s turn into a practical application of fairness in AI. We will be testing the ‘fair-ml’ algorithms developed by IBM Research, available in their inFairness package. These algorithms are designed with a focus on fairness, guided by the fairness metric proposed by Dwork et al., 2011. To explore these implementation we will follow the model example provided in the package. We are going to work with Adult dataset (Dua \u0026 Graff, 2017) used to predict whether income exceeds $50K/yr based on census data. Also known as “Census Income” dataset Train dataset contains 13 features and 30178 observations. Test dataset contains 13 features and 15315 observations. Target column is a binary factor where 1: \u003c=50K and 2: \u003e50K for annual income. ","date":"2023-06-01","objectID":"/responsible_ai/:4:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Libraries import torch import torch.nn as nn import torch.nn.functional as F from tqdm.auto import tqdm from inFairness.fairalgo import SenSeI from inFairness import distances from inFairness.auditor import SenSRAuditor, SenSeIAuditor %load_ext autoreload %autoreload 2 import metrics ","date":"2023-06-01","objectID":"/responsible_ai/:4:1","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Bias exploration import pandas as pd url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data' names = [ 'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'annual-income' ] data = pd.read_csv(url, sep=',', names=names) What are the 'Adult' dataset features? data.head() age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country annual-income 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States \u003c=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States \u003c=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States \u003c=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States \u003c=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba \u003c=50K data['annual-income'].value_counts() annual-income \u003c=50K 24720 \u003e50K 7841 Name: count, dtype: int64 The dataset is imbalanced: 25% make at least $50k per year. This imbalanced also appears in sex and race as shown here: (imbal_sex := data.groupby(['annual-income', 'sex']).size() .sort_values(ascending=False) .reset_index(name='count') .assign(percentage = lambda df:100 * df['count']/df['count'].sum()) ) annual-income sex count percentage 0 \u003c=50K Male 15128 46.460490 1 \u003c=50K Female 9592 29.458555 2 \u003e50K Male 6662 20.460060 3 \u003e50K Female 1179 3.620896 (imbal_race := data.groupby(['annual-income', 'race']).size() .sort_values(ascending=False) .reset_index(name='count') .assign(percentage = lambda df:100 * df['count']/df['count'].sum()) ) annual-income race count percentage 0 \u003c=50K White 20699 63.569915 1 \u003e50K White 7117 21.857437 2 \u003c=50K Black 2737 8.405761 3 \u003c=50K Asian-Pac-Islander 763 2.343294 4 \u003e50K Black 387 1.188538 5 \u003e50K Asian-Pac-Islander 276 0.847640 6 \u003c=50K Amer-Indian-Eskimo 275 0.844569 7 \u003c=50K Other 246 0.755505 8 \u003e50K Amer-Indian-Eskimo 36 0.110562 9 \u003e50K Other 25 0.076779 ","date":"2023-06-01","objectID":"/responsible_ai/:4:2","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Simple neural network model Folowing the IBM example in the income prediction task, we will test a simple neural network. class AdultDataset(Dataset): def __init__(self, data, labels): self.data = data self.labels = labels def __getitem__(self, idx): data = self.data[idx] label = self.labels[idx] return data, label def __len__(self): return len(self.labels) Note that the categorical variable are transformed into one-hot variables. import data train_df, test_df = data.load_data() X_train_df, Y_train_df = train_df X_test_df, Y_test_df = test_df X_train_df.head(1) age capital-gain capital-loss education-num hours-per-week marital-status_Divorced marital-status_Married-AF-spouse marital-status_Married-civ-spouse marital-status_Married-spouse-absent marital-status_Never-married ... relationship_Unmarried relationship_Wife sex_Male workclass_Federal-gov workclass_Local-gov workclass_Private workclass_Self-emp-inc workclass_Self-emp-not-inc workclass_State-gov workclass_Without-pay 0 0.409331 -0.14652 -0.218253 -1.613806 -0.49677 False False False False True ... True False False False False True False False False False In the IBM-inFairness model example the protected attributes are dropped from the training and test data. That is usually the case in fairness-aware machine learning models,especially when dealing with known biased features. The aim is to prevent the model from directly using these sensitive attributes for decision-making, thereby avoiding potential discriminatory outcomes. However, this approach has some limitations. Even when the protected attributes are removed, other features in the dataset might act as proxies for it, potentially retaining a strong signal of the biased information. As an example certain occupations, neighborhoods, or education levels might be disproportionately associated with certain racial groups due to societal factors. So, even without explicit information about race, the model might still end up learning patterns that indirectly reflect racial biases. On the other hand, removing sensitives attributes makes it difficult to analyze the fairness of the model. If we don’t know the race of the individuals in our dataset, we can’t check whether our model is treating individuals of different races equally. In some cases, it’s important to consider sensitive attributes to ensure fairness. For example, in order to correct for historical biases or to achieve certain diversity and inclusion goals, it might be necessary to consider these attributes. So, while removing sensitive attributes might seem like an easy fix, it doesn’t necessarily solve the problem of bias and might introduce new problems. Instead, it’s often better to use techniques that aim to ensure that the model treats similar individuals similarly (individual fairness), regardless of their sensitive attributes. protected_vars = ['race_White', 'sex_Male'] X_protected_df = X_train_df[protected_vars] X_train_df = X_train_df.drop(columns=protected_vars) X_test_df = X_test_df.drop(columns=protected_vars) In assessing individual fairness, the example we are working with implements a variable consistency measure using the ‘spouse’ attribute. This involves flipping the ‘spouse’ variable in the dataset, essentially simulating a scenario where individuals with the same characteristics but different ‘spouse’ values are compared. The goal is to ensure that the model’s predictions are consistent for individuals who are similar except for their ‘spouse’ attribute, thereby upholding the principle of individual fairness. This approach provides a practical way to audit the model’s fairness by checking if similar individuals are treated similarly. X_test_df.relationship_Wife.values.astype(int) array([0, 1, 0, ..., 0, 0, 0]) X_test_df_spouse_flipped = X_test_df.copy() X_test_df_spouse_flipped.relationship_Wife = 1 - X_test_df_spouse_flipped.relationship_Wife X_test_df_spouse_flipped.relationship_Wife.values array([1, 0, 1, ..., 1, 1, 1]) device = torch.devic","date":"2023-06-01","objectID":"/responsible_ai/:4:3","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Standard training input_size = X_train.shape[1] output_size = 2 network_standard = Model(input_size, output_size).to(device) optimizer = torch.optim.Adam(network_standard.parameters(), lr=1e-3) loss_fn = F.cross_entropy EPOCHS = 10 network_standard.train() for epoch in tqdm(range(EPOCHS)): for x, y in train_dl: x, y = x.to(device), y.to(device) optimizer.zero_grad() y_pred = network_standard(x).squeeze() loss = loss_fn(y_pred, y) loss.backward() optimizer.step() 100%|██████████| 10/10 [00:08\u003c00:00, 1.24it/s] accuracy = metrics.accuracy(network_standard, test_dl, device) balanced_acc = metrics.balanced_accuracy(network_standard, test_dl, device) spouse_consistency = metrics.spouse_consistency(network_standard, test_dl, test_dl_flip, device) print(f'Accuracy: {accuracy}') print(f'Balanced accuracy: {balanced_acc}') print(f'Spouse consistency: {spouse_consistency}') Accuracy: 0.8555948734283447 Balanced accuracy: 0.7764129391420478 Spouse consistency: 0.9636222910216719 The simple NN achieve .85 of accuracy. However, the inconsistency score of 0.04 on the ‘spouse’ variable suggests that the model is not treating similar individuals consistently, which is a violation of individual fairness. This inconsistency could be due to the fact that the model is learning to differentiate based on gender, despite the intention to avoid such bias. ","date":"2023-06-01","objectID":"/responsible_ai/:4:4","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Individually fair training with LogReg fair metric In the following section, a fair machine learning model is introduced. This model is said to be fair because its performance remains consistent under certain perturbations within a sensitive subspace, meaning it is robust to partial data variations. To illustrate the authors’ approach, let’s consider the process of evaluating the fairness of a resume screening system. An auditor might alter the names on resumes of applicants from the ethnic majority group to those more commonly found among the ethnic minority group. If the system’s performance declines upon reviewing the altered resumes (i.e., the evaluations become less favorable), one could infer that the model exhibits bias against applicants from the ethnic minority group. To algorithmically address this issue, the authors propose a method to instill individual fairness during the training of ML models. This is achieved through distributionally robust optimization (DRO), an optimization technique that seeks the optimal solution while considering a fairness metric (inspired by Adversarial Robustness). ","date":"2023-06-01","objectID":"/responsible_ai/:4:5","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Learning fair metric from data and its hidden signals The authors use Wasserstein distances to measure the similarity between individuals. Unlike Mahalanobis, Wasserstein distance can be used to compare two probability distributions and is defined as the minimum cost that must be paid to transform one distribution into the other. The distances between data points are calculated in a way that takes into account protected attributes (in our example: gender or race). The goal is to ensure that similar individuals, as determined by Wasserstein distance, are treated similarly by the machine learning model. To achieve this, the algorithm learn ‘sensitive directions’ in the data. These are directions in the feature space along which changes are likely to correspond to changes in protected attributes. These is a clever approach to uncover hidden biases by identifying subtle patterns that may correspond to changes in protected attributes, even if those attributes are not present in our model inputs. This allows the model to account for potential biases that might otherwise go unnoticed. For instance, to identify a sensitive direction associated with a particular attribute (e.g., gender), the algorithm use a logistic regression classifier to distinguish between classes (such as men and women in the data). The coefficients from this logistic regression model define a direction within the feature space. The performance of the machine learning model is assessed by its worst-case performance on hypothetical populations of users with perturbed sensitive attributes. By minimizing the loss function, the system is ensured to perform well on all such populations. # Same architecture we found network_fair_LR = Model(input_size, output_size).to(device) optimizer = torch.optim.Adam(network_fair_LR.parameters(), lr=1e-3) lossfn = F.cross_entropy # set the distance metric for instances similiraty detections distance_x_LR = distances.LogisticRegSensitiveSubspace() distance_y = distances.SquaredEuclideanDistance() # train fair metric distance_x_LR.fit(X_train, data_SensitiveAttrs=X_protected) distance_y.fit(num_dims=output_size) distance_x_LR.to(device) distance_y.to(device) rho = 5.0 eps = 0.1 auditor_nsteps = 100 auditor_lr = 1e-3 fairalgo_LR = SenSeI(network_fair_LR, distance_x_LR, distance_y, lossfn, rho, eps, auditor_nsteps, auditor_lr) ","date":"2023-06-01","objectID":"/responsible_ai/:4:6","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"A fair objective function The objective function that is minimized during the training of a fair machine learning model as proposed in the inFairness package is composed of two parts: the loss function and the fair metric (see SenSeI): fair_loss = torch.mean( #--------1------------- + -----------------2-----------------------------# self.loss_fn(Y_pred, Y) + self.rho * self.distance_y(Y_pred, Y_pred_worst) ) Loss Function: a classical loss function that measure of how well the model’s predictions match the actual data. The goal of this metric is to adjust the model’s parameters to minimize the loss score, and Fair Metric (DIF): the fairness term is a measure of the difference between the model’s predictions on the original data and its predictions on the worst-case examples. The model is trying to minimize this objective function, which means it’s trying to make accurate and fair predictions. It’s important to note that due to the computation of a complex loss score, the training process becomes more resource-intensive. fairalgo_LR.train() for epoch in tqdm(range(EPOCHS)): for x, y in train_dl: x, y = x.to(device), y.to(device) optimizer.zero_grad() result = fairalgo_LR(x, y) result.loss.backward() optimizer.step() 100%|██████████| 10/10 [10:09\u003c00:00, 60.90s/it] accuracy = metrics.accuracy(network_fair_LR, test_dl, device) balanced_acc = metrics.balanced_accuracy(network_fair_LR, test_dl, device) spouse_consistency = metrics.spouse_consistency(network_fair_LR, test_dl, test_dl_flip, device) print(f'Accuracy: {accuracy}') print(f'Balanced accuracy: {balanced_acc}') print(f'Spouse consistency: {spouse_consistency}') Accuracy: 0.8401150107383728 Balanced accuracy: 0.742399333699871 Spouse consistency: 0.9997788589119858 ","date":"2023-06-01","objectID":"/responsible_ai/:4:7","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Results # Auditing using the SenSR Auditor + LR metric audit_nsteps = 1000 audit_lr = 0.1 auditor_LR = SenSRAuditor(loss_fn=loss_fn, distance_x=distance_x_LR, num_steps=audit_nsteps, lr=audit_lr, max_noise=0.5, min_noise=-0.5) audit_result_stdmodel = auditor_LR.audit(network_standard, X_test, y_test, lambda_param=10.0, audit_threshold=1.15) audit_result_fairmodel_LR = auditor_LR.audit(network_fair_LR, X_test, y_test, lambda_param=10.0, audit_threshold=1.15) print(\"=\"*100) print(\"LR metric\") print(f\"Loss ratio (Standard model) : {audit_result_stdmodel.lower_bound}. Is model fair: {audit_result_stdmodel.is_model_fair}\") print(f\"Loss ratio (fair model - LogReg metric) : {audit_result_fairmodel_LR.lower_bound}. Is model fair: {audit_result_fairmodel_LR.is_model_fair}\") LR metric Loss ratio (Standard model) : 2.1810670575586046. Is model fair: False Loss ratio (fair model - LogReg metric) : 1.0531351204682995. Is model fair: True Findings Upon reviewing the overall results, we see that with a minor decrease in accuracy of 0.01, we have successfully constructed a fair model that is debiased with respect to gender and race. ","date":"2023-06-01","objectID":"/responsible_ai/:4:8","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Conclusion We’ve explored a specific example of a fair-ML model using the tools provided by the inFairness package. While the results are promising, it’s important to contextualize them within the broader challenges of responsible AI, particularly given the rapid evolution of ML tools and the dynamic nature of societal values. Following Richardson, we can mention: Conflicting Fairness Metrics: Measurement is always a political activity in the sense that we must select, define, and prioritize certain dimensions of reality, setting aside others. Friedler et al. (2021) argue that fairness experts must explicitly state the priorities of each fairness metric to ensure practitioners make informed choices. Metric Robustness: Friedler et al. (2018) discovered that many fairness metrics lack robustness. Their study showed that by simply modifying dataset composition and changing train-test splits, many fairness criteria lacked stability. Oversimplification of Fairness: A major concern in the literature is the emphasis on technical solutions to algorithmic bias, which is a socio-technical problem. Madaio et al. (2020) referred to the exclusive use of technical solutions as “ethics washing,” and Selbst et al. (2019) describe the failure to recognize that fairness cannot be solely achieved through mathematical formulation as the “formalism trap.” Operationalization of Ethical Concepts: A significant challenge for fair AI is translating ethical reflections into actionable products and procedures that practitioners and institutions can implement. This difficulty is not unique to the AI field but affects every aspect of human activity where there is a need for ethical actions. Bibliography ","date":"2023-06-01","objectID":"/responsible_ai/:5:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Responsible AI"],"content":"Bibliography Dwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. “Fairness Through Awareness.” arXiv, November 28, 2011. https://doi.org/10.48550/arXiv.1104.3913. Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. “A Survey on Bias and Fairness in Machine Learning.” arXiv, January 25, 2022. https://doi.org/10.48550/arXiv.1908.09635. Richardson, Brianna, and Juan E. Gilbert. “A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions.” arXiv, December 10, 2021. https://doi.org/10.48550/arXiv.2112.05700. Weerts, Hilde, Miroslav Dudík, Richard Edgar, Adrin Jalali, Roman Lutz, and Michael Madaio. “Fairlearn: Assessing and Improving Fairness of AI Systems.” arXiv, March 29, 2023. https://doi.org/10.48550/arXiv.2303.16626. Bird, Sarah., Dudík, Miro., Edgar, Richard., Horn, Brandon., Lutz, Roman., Milan, Vanessa., Sameki, Mehrnoosh., Wallach, Hanna., \u0026 Walker, Kathleen. “Fairlearn: A toolkit for assessing and improving fairness in AI.” Microsoft, May 2020. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai. Floridi, Luciano., Cowls, Josh., Beltrametti, Monica., Chatila, Raja., Chazerand, Patrice., Dignum, Virginia., Luetge, Christoph., Madelin, Robert., Pagallo, Ugo., Rossi, Francesca., Schafer, Burkhard., Valcke, Peggy., \u0026 Vayena, Effy. “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds and Machines, December 1, 2018. https://doi.org/10.1007/s11023-018-9482-5. Weerts, Hilde, Miroslav Dudík, Richard Edgar, Adrin Jalali, Roman Lutz, and Michael Madaio. “Fairlearn: Assessing and Improving Fairness of AI Systems.” arXiv, March 29, 2023. https://doi.org/10.48550/arXiv.2303.16626. Yurochkin, Mikhail, Amanda Bower, and Yuekai Sun. “Training Individually Fair ML Models with Sensitive Subspace Robustness.” arXiv, March 13, 2020. http://arxiv.org/abs/1907.00020. Pic by Patrick Fore, Unsplash ","date":"2023-06-01","objectID":"/responsible_ai/:6:0","tags":["fairness","bias","neural networks"],"title":"Responsible AI Tools","uri":"/responsible_ai/"},{"categories":["Dashboards","Data driven culture"],"content":"This post chronicles the transformative journey that the Judiciary of Entre Ríos, Argentina, where I serve as Director, is undertaking in the realm of statistics. ","date":"2016-12-01","objectID":"/analitics_in_judiciary/:0:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/analitics_in_judiciary/"},{"categories":["Dashboards","Data driven culture"],"content":"2016 Creation of a Specialized Body The Area of Planning, Management and Statistics, created in June 2016, undertook the project of reforming the judicial statistics of the province, beset by serious difficulties, through a profound change in working tools and more broadly in organizational culture (i.e. from the digitization of information and the design of new indicators, to the creation of a new calendar of statistical processes and internal regulations). It had the collaboration of magistrates and officials from all over the province who provided the substantive knowledge necessary for the generation of reliable judicial metrics. Despite limitations in tools and staff, efficient solutions were implemented, such as the use of the open-source software R-Statistical Computing for data processing. In a span of 10 months, judicial statistics were revolutionized, optimizing report production times, introducing new indicators, and launching a new Public Judicial Statistics System, which extended to all branches of provincial justice by June 2018 (more info). ","date":"2016-12-01","objectID":"/analitics_in_judiciary/:1:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/analitics_in_judiciary/"},{"categories":["Dashboards","Data driven culture"],"content":"2019 Public Dashboards After two challenging years of work, the Statistics Dashboards of the Judicial Power of Entre Ríos were made available online. On December 17, 2019, the provincial justice system presented to society a public access tool to judicial indicators based on a formal production system, entirely supported by the open-source software R-Statistical Computing. Statistics Dashboards With this presentation, the team I was fortunate to lead fulfilled a long-sought goal in terms of open judicial data, creating along the way a unique model of judicial statistics in Argentina (more info). ","date":"2016-12-01","objectID":"/analitics_in_judiciary/:2:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/analitics_in_judiciary/"},{"categories":["Dashboards","Data driven culture"],"content":"2021 Judicial Activity in COVID19 Pandemic I made a brief internal presentation on the impact of the Covid-19 Pandemic on judicial activity, with some details about the number of processes carried out between 2020 and 2021, which had a significant impact as it dispelled doubts about the activity of justice and the importance of its services (more info). After that, I gave a presentation on local television about the situation of the Judiciary during the sanitary isolation of the Pandemic: Finally, by the end of that year, we were already sharing our statistical transformation experience with all Judicial Powers in our country, gathered in the Federal Council of Courts and Superior Courts of Argentina (more info). ","date":"2016-12-01","objectID":"/analitics_in_judiciary/:3:0","tags":["Judiciary","Public Service"],"title":"Justice Analitics","uri":"/analitics_in_judiciary/"},{"categories":null,"content":"The aplications that I've build","date":"2019-08-02","objectID":"/aplications/","tags":null,"title":"Aplications and Tools","uri":"/aplications/"},{"categories":null,"content":"Justice Dashboard The Justice Dashboard is the online service of the Supreme Court of Entre Ríos, Argentina, to access primary data and justice statistics. It is based on free software R and Shiny and follows the standards of the open data movement. The central part of the system, the statistical scripts, are accessible in operations, proccesing, organization and research. Justice Data ","date":"2019-08-02","objectID":"/aplications/:1:0","tags":null,"title":"Aplications and Tools","uri":"/aplications/"},{"categories":null,"content":"Features  Optimized for public access and reproducibility of statistics.  Compliance with national and international standards on public information and sensitive data.  Full user access of algorithms and scripts. ","date":"2019-08-02","objectID":"/aplications/:1:1","tags":null,"title":"Aplications and Tools","uri":"/aplications/"},{"categories":null,"content":"Airports Worldwide The Airports Worldwide app is a pet-project developed for the Universidad Tecnológica Nacional, Visualizations seminar, embodying a database of airports around the globe. Build with Python and Dash, it presents an interactive and user-friendly interface for users to explore the data. It is deploy on Heroku. Airports ","date":"2019-08-02","objectID":"/aplications/:2:0","tags":null,"title":"Aplications and Tools","uri":"/aplications/"},{"categories":null,"content":"Features  Worlwide Database: The application provides information about airports worldwide. Users can access specifics information as location, airport size and more.  Interactive: the application enabling users to visualize the geographical distribution of airports across the world. Users can zoom in and out to view airports in specific regions or countries, making it easy to understand global airport infrastructure. ","date":"2019-08-02","objectID":"/aplications/:2:1","tags":null,"title":"Aplications and Tools","uri":"/aplications/"},{"categories":null,"content":"Predictive Modeling in Justice The Predictive Modeling Series (Parts: 1, 2, 3 and 4) showcases our development of models for time series data in judicial services.Our aim is to predict the number of rulings per subject matter over a specified period. For this project, we’ve exclusively utilized R, specifically leveraging the tidymodels package. credit:Aron Visuals ","date":"2019-08-02","objectID":"/aplications/:3:0","tags":null,"title":"Aplications and Tools","uri":"/aplications/"},{"categories":null,"content":"Features  The complete scripts of this implementation can be accessed here. ","date":"2019-08-02","objectID":"/aplications/:3:1","tags":null,"title":"Aplications and Tools","uri":"/aplications/"}]