<!DOCTYPE html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Generative Models: Variational Autoencoders - CastilloS</title><meta name="Description" content="This is my cool site"><meta property="og:url" content="http://localhost:1313/variational_autoencoders/">
  <meta property="og:site_name" content="CastilloS">
  <meta property="og:title" content="Generative Models: Variational Autoencoders">
  <meta property="og:description" content="Generative models are a class of statistical models that aim to learn the underlying data distribution from a given dataset. These models provide a way to generate new samples that are statistically similar to the training data. They have gained substantial attention in various domains, such as image generation, speech synthesis, and even drug discovery.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-09-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-09-01T00:00:00+00:00">
    <meta property="article:tag" content="Expectation Maximization">
    <meta property="article:tag" content="Variational Autoencoders">
    <meta property="article:tag" content="Deep Neural Networks">
    <meta property="og:image" content="http://localhost:1313/variational_autoencoders/featured-image.jpg">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/variational_autoencoders/featured-image.jpg">
  <meta name="twitter:title" content="Generative Models: Variational Autoencoders">
  <meta name="twitter:description" content="Generative models are a class of statistical models that aim to learn the underlying data distribution from a given dataset. These models provide a way to generate new samples that are statistically similar to the training data. They have gained substantial attention in various domains, such as image generation, speech synthesis, and even drug discovery.">
      <meta name="twitter:site" content="@clausecastillo">
<meta name="application-name" content="castillos">
<meta name="apple-mobile-web-app-title" content="castillos"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/variational_autoencoders/" /><link rel="prev" href="http://localhost:1313/documents_qa/" /><link rel="next" href="http://localhost:1313/genetic_algorithms/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/css/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Generative Models: Variational Autoencoders",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/variational_autoencoders\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "http:\/\/localhost:1313\/variational_autoencoders\/featured-image.jpg",
                            "width":  6067 ,
                            "height":  3467 
                        }],"genre": "posts","keywords": "expectation maximization, variational autoencoders, deep neural networks","wordcount":  2234 ,
        "url": "http:\/\/localhost:1313\/variational_autoencoders\/","datePublished": "2023-09-01T00:00:00+00:00","dateModified": "2023-09-01T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "xxxx"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="CastilloS">CastilloS</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/aplications/"> Aplications </a><a class="menu-item" href="https://github.com/castillosebastian/castillocs" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a><a href="javascript:void(0);" class="menu-item language" title="Select Language">
                    <i class="fa fa-globe fa-fw" aria-hidden="true"></i>                      
                    <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/variational_autoencoders/" selected>English</option></select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="CastilloS">CastilloS</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/aplications/" title="">Aplications</a><a class="menu-item" href="https://github.com/castillosebastian/castillocs" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a><a href="javascript:void(0);" class="menu-item language" title="Select Language">
                    <i class="fa fa-globe fa-fw" aria-hidden="true"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/variational_autoencoders/" selected>English</option></select>
                </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content always-active" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Generative Models: Variational Autoencoders</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>xxxx</a></span>&nbsp;<span class="post-category">included in <a href="/categories/generative-models/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Generative Models</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-09-01">2023-09-01</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2234 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;11 minutes&nbsp;</div>
        </div><div class="featured-image"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/variational_autoencoders/featured-image.jpg"
        data-srcset="/variational_autoencoders/featured-image.jpg, /variational_autoencoders/featured-image.jpg 1.5x, /variational_autoencoders/featured-image.jpg 2x"
        data-sizes="auto"
        alt="/variational_autoencoders/featured-image.jpg"
        title="/variational_autoencoders/featured-image.jpg" /></div><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#generative-model">Generative Model</a></li>
    <li><a href="#the-challenge-of-maximum-likelihood-estimates-mle-for-unseen-observations">The Challenge of Maximum Likelihood Estimates (MLE) for Unseen Observations</a></li>
    <li><a href="#overcoming-the-challenge-with-expectation-maximization-em">Overcoming the Challenge with Expectation Maximization (EM)</a></li>
    <li><a href="#variational-autoencoders-vaes">Variational Autoencoders (VAEs)</a></li>
    <li><a href="#variational-encoders-with-pytorch">Variational Encoders with Pytorch</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Generative models are a class of statistical models that aim to learn the underlying data distribution from a given dataset. These models provide a way to generate new samples that are statistically similar to the training data. They have gained substantial attention in various domains, such as image generation, speech synthesis, and even drug discovery.</p>
<h2 id="generative-model">Generative Model</h2>
<p>Generative models are a class of statistical models that aim to learn the underlying data distribution. Given a dataset of observed samples, one starts by selecting a distributional model parameterized by $(\theta)$. The objective is to estimate $(\theta)$ such that it aligns optimally with the observed samples.The anticipation is that it can also generalize to samples outside the training set.</p>
<p>The optimal distribution is hence the one that maximizes the likelihood of producing the observed data, giving lower probabilities to infrequent observations and higher probabilities to the more common ones (the principle underlying this assumption is that &rsquo;the world is a boring place&rsquo; -in words of Bhiksha Raj-).</p>
<h2 id="the-challenge-of-maximum-likelihood-estimates-mle-for-unseen-observations">The Challenge of Maximum Likelihood Estimates (MLE) for Unseen Observations</h2>
<p>When training generative models, a natural objective is to optimize the model parameters such that the likelihood of the observed data under the model is maximized. This method is known as <strong>Maximum Likelihood Estimation (MLE)</strong>. In mathematical terms, given observed data $X$, the MLE seeks parameters $\theta$ that maximize:</p>
<blockquote>
<p>$$p_\theta(X)$$</p></blockquote>
<p>However, for many generative models, especially those that involve latent or unobserved variables, the likelihood term involves summing or integrating over all possible configurations of these latent variables. Mathematically, this turns into:</p>
<blockquote>
<p>$$p_\theta(X) = \sum_{Z} p_\theta(X,Z)$$ $$or$$ $$p_\theta(X) = \int p_\theta(X,Z) dZ$$</p></blockquote>
<p>Computing the log-likelihood, which is often used for numerical stability and optimization ease, leads to a log of summations (for discrete latent variables) or a log of integrals (for continuous latent variables):</p>
<blockquote>
<p>$$log p_\theta(X) = \log \sum_{Z} p_\theta(X,Z)$$ $$or$$ $$log p_\theta(X) = \log \int p_\theta(X,Z) dZ$$</p></blockquote>
<p>These expressions are typically intractable to optimize directly due to the presence of the log-sum or log-integral operations (see the info below).</p>
<div class="details admonition info">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-info-circle fa-fw" aria-hidden="true"></i>Marginalization in Joint Probability<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><h2 id="marginalization-in-the-context-of-joint-probability">Marginalization in the Context of Joint Probability</h2>
<p>When discussing the computation of the joint probability for observed and missing data, the term &ldquo;marginalizing&rdquo; refers to summing or integrating over all possible outcomes of the missing data. This process provides a probability distribution based solely on the observed data. For example, let&rsquo;s assume:</p>
<ul>
<li>$X$ is the observed data</li>
<li>$Z$ is the missing data</li>
<li>The joint probability for both is represented as $p(X,Z)$</li>
</ul>
<p>If your primary interest lies in the distribution of $X$ and you wish to eliminate the dependence on $Z$, you&rsquo;ll need to carry out marginalization for $Z$. For discrete variables, the marginalization involves the logarithm of summation, and for continuous variables, it pertains to integration. In any case, functions that includes the log of a sum o integral defies direct optimization.</p>
</div>
        </div>
    </div>
<p>Can we get an approximation to this that is more tractable (without a summation or integral within the log)?</p>
<h2 id="overcoming-the-challenge-with-expectation-maximization-em">Overcoming the Challenge with Expectation Maximization (EM)</h2>
<p>To address the optimization challenge in MLE with latent variables, the <strong>Expectation Maximization (EM)</strong> algorithm is employed. The EM algorithm offers a systematic approach to iteratively estimate both the model parameters and the latent variables.</p>
<p>The algorithm involves two main steps:</p>
<ol>
<li><strong>E-step (Expectation step)</strong>: involves computing the expected value of the complete-data log-likelihood with respect to the posterior distribution of the latent variables given the observed data.</li>
<li><strong>M-step (Maximization step)</strong>: Update the model parameters to maximize this expected log-likelihood from the E-step.</li>
</ol>
<p>By alternating between these two steps, EM ensures that the likelihood increases with each iteration until convergence, thus providing a practical method to fit generative models with latent variables.</p>
<p>For E-step the <strong>Variational Lower Bound</strong> is used. Commonly referred to as the Empirical Lower BOund (ELBO), is a central concept in variational inference. This method is used to approximate complex distributions (typically posterior distributions) with simpler, more tractable ones. The ELBO is an auxiliary function that provides a lower bound to the log likelihood of the observed data. By iteratively maximizing the ELBO with respect to variational parameters, we approximate the Maximum Likelihood Estimation (MLE) of the model parameters.</p>
<p>Let&rsquo;s reconsider our aim to maximize the log-likelihood of observations $x$ in terms of $q_\phi(z|x)$.</p>
<blockquote>
<p>$$\log p_\theta(x) = \log \int z p_\theta(x,z)dz$$ $$ = \log \int z \frac{p_\theta(x,z)q_\phi(z|x)}{q_\phi(z|x)}dz$$ $$= \log E_{z \sim q_\phi(z|x)} \left[ \frac{p_\theta(x,z)}{q_\phi(z|x)} \right]$$ $$\geq E_z \left[ \log \frac{p_\theta(x,z)}{q_\phi(z|x)} \right] \quad \text{(by Jensen&rsquo;s inequality)}$$ $$= E_z[\log p_\theta(x,z)] + \int z q_\phi(z|x) \log \frac{1}{q_\phi(z|x)} dz$$ $$= E_z[\log p_\theta(x,z)] + H(q_\phi(z|x))$$</p></blockquote>
<p>In the equation above, the term $H(\cdot)$ denotes the Shannon entropy. By definition, the term &ldquo;evidence&rdquo; is the value of a likelihood function evaluated with fixed parameters. With the definition of:</p>
<blockquote>
<p>$$L = E_z[\log p_\theta(x,z)] + H(q_\phi(z|x)),$$</p></blockquote>
<p>it turns out that $L$ sets a lower bound for the evidence of observations and maximizes $L$ will push up the log-likelihood of $x$.</p>
<h2 id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)</h2>
<p>Variational Autoencoders are a specific type of generative model that brings together ideas from deep learning and Bayesian inference. VAEs are especially known for their application in generating new, similar data to the input data (like images or texts) and for their ability to learn latent representations of data.</p>
<p><strong>1. Generative Models and Latent Variables</strong></p>
<p>In generative modeling, our goal is to learn a model of the probability distribution from which a dataset is drawn. The model can then be used to generate new samples. A VAE makes a specific assumption that there exist some <em>latent variables</em> (or hidden variables) that when transformed give rise to the observed data.</p>
<p>Let $x$ be the observed data and $z$ be the latent variables. The generative story can be seen as:</p>
<ol>
<li>Draw $z$ from a prior distribution, $p(z)$.</li>
<li>Draw $x$ from a conditional distribution, $p(x|z)$.</li>
</ol>
<p><strong>2. Problem of Direct Inference</strong></p>
<p>As discussed previously, direct inference for the posterior distribution $p(z|x)$ (i.e., the probability of the latent variables given the observed data) can be computationally challenging, especially when dealing with high-dimensional data or complex models. This is because:</p>
<blockquote>
<p>$$ p(z|x) = \frac{p(x|z) p(z)}{p(x)} $$</p></blockquote>
<p>Here, $p(x)$ is the evidence (or marginal likelihood) which is calculated as:</p>
<blockquote>
<p>$$ p(x) = \int p(x|z) p(z) dz $$</p></blockquote>
<p>As we saw this integral is intractable for most interesting models.</p>
<p><strong>3. Variational Inference and ELBO</strong></p>
<p>To sidestep the intractability of the posterior, VAEs employ <em>variational inference</em>. Instead of computing the posterior directly, we introduce a parametric approximate posterior distribution, $q_{\phi}(z|x)$, with its own parameters $\phi$.</p>
<p>The goal now shifts to making this approximation as close as possible to the true posterior. This is done by minimizing the <em>Kullback-Leibler divergence</em> between the approximate and true posterior using the ELBO function.</p>
<p><strong>4. Neural Networks and Autoencoding Structure</strong></p>
<p>In VAEs, neural networks are employed to parameterize the complex functions. Specifically:</p>
<ol>
<li><strong>Encoder Network</strong>: This maps the observed data, $x$, to the parameters of the approximate posterior, $q_{\phi}(z|x)$.</li>
<li><strong>Decoder Network</strong>: Given samples of $z$ drawn from $q_{\phi}(z|x)$, this maps back to the data space, outputting parameters for the data likelihood, $p_{\theta}(x|z)$.</li>
</ol>
<p>The &ldquo;autoencoder&rdquo; terminology comes from the encoder-decoder structure where the model is trained to reconstruct its input data.</p>
<p><strong>5. Training a VAE</strong></p>
<p>The training process involves:</p>
<ol>
<li><strong>Forward pass</strong>: Input data is passed through the encoder to obtain parameters of $q_{\phi}(z|x)$.</li>
<li><strong>Sampling</strong>: Latent variables $z$ are sampled from $q_{\phi}(z|x)$ using the <em>reparameterization trick</em> for backpropagation.</li>
<li><strong>Reconstruction</strong>: The sampled $z$ values are passed through the decoder to obtain the data likelihood parameters, $p_{\theta}(x|z)$.</li>
<li><strong>Loss Computation</strong>: Two terms are considered - reconstruction loss (how well the VAE reconstructs the data) and the KL divergence between $q_{\phi}(z|x)$ and $p(z)$.</li>
<li><strong>Backpropagation and Optimization</strong>: The model parameters $\phi$ and $\theta$ are updated to maximize the ELBO.</li>
</ol>
<p>By the end of the training, you&rsquo;ll have a model that can generate new samples resembling your input data by simply sampling from the latent space and decoding the samples.</p>
<p>VAEs are a powerful tools, that stay in the intersection of deep learning and probabilistic modeling, and they have a plethora of applications, especially in unsupervised learning tasks.</p>
<h2 id="variational-encoders-with-pytorch">Variational Encoders with Pytorch</h2>
<p>Let create a basic implementation of a Variational Autoencoder (VAE) using PyTorch. The VAE will be designed to work on simple image data, such as the MNIST dataset.</p>
<div class="code-block code-line-numbers" style="counter-reset: code-block 0">
    <div class="code-header language-python">
        <span class="code-title"><i class="arrow fas fa-chevron-right fa-fw" aria-hidden="true"></i></span>
        <span class="ellipses"><i class="fas fa-ellipsis-h fa-fw" aria-hidden="true"></i></span>
        <span class="copy" title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden="true"></i></span>
    </div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the VAE architecture</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">VAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Encoder</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>  <span class="c1"># mu</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>  <span class="c1"># logvar</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Decoder</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>   <span class="c1"># Add this line</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="o">*</span><span class="n">std</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Loss function: Reconstruction + KL Divergence Losses summed over all elements and batch</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">BCE</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># KLD = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)</span>
</span></span><span class="line"><span class="cl">    <span class="n">KLD</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">BCE</span> <span class="o">+</span> <span class="n">KLD</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Epoch: </span><span class="si">{}</span><span class="s1"> [</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1"> (</span><span class="si">{:.0f}</span><span class="s1">%)]</span><span class="se">\t</span><span class="s1">Loss: </span><span class="si">{:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====&gt; Epoch: </span><span class="si">{}</span><span class="s1"> Average loss: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">comparison</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">recon_batch</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)[:</span><span class="n">n</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">                <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">save_image</span><span class="p">(</span><span class="n">comparison</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="s1">&#39;reconstruction_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.png&#39;</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;====&gt; Test set loss: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Run the training loop</span>
</span></span><span class="line"><span class="cl"><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">test</span><span class="p">()</span></span></span></code></pre></div></div>
<div class="code-block code-line-numbers" style="counter-reset: code-block 0">
    <div class="code-header language-">
        <span class="code-title"><i class="arrow fas fa-chevron-right fa-fw" aria-hidden="true"></i></span>
        <span class="ellipses"><i class="fas fa-ellipsis-h fa-fw" aria-hidden="true"></i></span>
        <span class="copy" title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden="true"></i></span>
    </div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Train Epoch: 1 [0/60000 (0%)]   Loss: 547.095459
</span></span><span class="line"><span class="cl">Train Epoch: 1 [12800/60000 (21%)]  Loss: 177.320297
</span></span><span class="line"><span class="cl">Train Epoch: 1 [25600/60000 (43%)]  Loss: 156.426804
</span></span><span class="line"><span class="cl">Train Epoch: 1 [38400/60000 (64%)]  Loss: 137.500916
</span></span><span class="line"><span class="cl">Train Epoch: 1 [51200/60000 (85%)]  Loss: 130.676682
</span></span><span class="line"><span class="cl">====&gt; Epoch: 1 Average loss: 164.3802
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 127.3049
</span></span><span class="line"><span class="cl">Train Epoch: 2 [0/60000 (0%)]   Loss: 129.183395
</span></span><span class="line"><span class="cl">Train Epoch: 2 [12800/60000 (21%)]  Loss: 124.367867
</span></span><span class="line"><span class="cl">Train Epoch: 2 [25600/60000 (43%)]  Loss: 119.659966
</span></span><span class="line"><span class="cl">Train Epoch: 2 [38400/60000 (64%)]  Loss: 120.912560
</span></span><span class="line"><span class="cl">Train Epoch: 2 [51200/60000 (85%)]  Loss: 114.011864
</span></span><span class="line"><span class="cl">====&gt; Epoch: 2 Average loss: 121.6398
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 115.7936
</span></span><span class="line"><span class="cl">Train Epoch: 3 [0/60000 (0%)]   Loss: 114.913048
</span></span><span class="line"><span class="cl">Train Epoch: 3 [12800/60000 (21%)]  Loss: 117.442482
</span></span><span class="line"><span class="cl">Train Epoch: 3 [25600/60000 (43%)]  Loss: 111.994392
</span></span><span class="line"><span class="cl">Train Epoch: 3 [38400/60000 (64%)]  Loss: 112.240242
</span></span><span class="line"><span class="cl">Train Epoch: 3 [51200/60000 (85%)]  Loss: 114.725128
</span></span><span class="line"><span class="cl">====&gt; Epoch: 3 Average loss: 114.6564
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 112.2248
</span></span><span class="line"><span class="cl">Train Epoch: 4 [0/60000 (0%)]   Loss: 110.638550
</span></span><span class="line"><span class="cl">Train Epoch: 4 [12800/60000 (21%)]  Loss: 114.595108
</span></span><span class="line"><span class="cl">Train Epoch: 4 [25600/60000 (43%)]  Loss: 109.188904
</span></span><span class="line"><span class="cl">Train Epoch: 4 [38400/60000 (64%)]  Loss: 111.060234
</span></span><span class="line"><span class="cl">Train Epoch: 4 [51200/60000 (85%)]  Loss: 114.594086
</span></span><span class="line"><span class="cl">====&gt; Epoch: 4 Average loss: 111.6810
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 109.6389
</span></span><span class="line"><span class="cl">Train Epoch: 5 [0/60000 (0%)]   Loss: 110.394012
</span></span><span class="line"><span class="cl">Train Epoch: 5 [12800/60000 (21%)]  Loss: 106.082031
</span></span><span class="line"><span class="cl">Train Epoch: 5 [25600/60000 (43%)]  Loss: 107.659363
</span></span><span class="line"><span class="cl">Train Epoch: 5 [38400/60000 (64%)]  Loss: 107.294495
</span></span><span class="line"><span class="cl">Train Epoch: 5 [51200/60000 (85%)]  Loss: 110.049332
</span></span><span class="line"><span class="cl">====&gt; Epoch: 5 Average loss: 109.9291
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 108.5438
</span></span><span class="line"><span class="cl">Train Epoch: 6 [0/60000 (0%)]   Loss: 106.701828
</span></span><span class="line"><span class="cl">Train Epoch: 6 [12800/60000 (21%)]  Loss: 109.286430
</span></span><span class="line"><span class="cl">Train Epoch: 6 [25600/60000 (43%)]  Loss: 110.426498
</span></span><span class="line"><span class="cl">Train Epoch: 6 [38400/60000 (64%)]  Loss: 106.086746
</span></span><span class="line"><span class="cl">Train Epoch: 6 [51200/60000 (85%)]  Loss: 106.020401
</span></span><span class="line"><span class="cl">====&gt; Epoch: 6 Average loss: 108.7957
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 107.6961
</span></span><span class="line"><span class="cl">Train Epoch: 7 [0/60000 (0%)]   Loss: 109.973251
</span></span><span class="line"><span class="cl">Train Epoch: 7 [12800/60000 (21%)]  Loss: 108.430046
</span></span><span class="line"><span class="cl">Train Epoch: 7 [25600/60000 (43%)]  Loss: 109.439484
</span></span><span class="line"><span class="cl">Train Epoch: 7 [38400/60000 (64%)]  Loss: 110.635895
</span></span><span class="line"><span class="cl">Train Epoch: 7 [51200/60000 (85%)]  Loss: 110.213860
</span></span><span class="line"><span class="cl">====&gt; Epoch: 7 Average loss: 107.9552
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 107.0711
</span></span><span class="line"><span class="cl">Train Epoch: 8 [0/60000 (0%)]   Loss: 108.046188
</span></span><span class="line"><span class="cl">Train Epoch: 8 [12800/60000 (21%)]  Loss: 105.081818
</span></span><span class="line"><span class="cl">Train Epoch: 8 [25600/60000 (43%)]  Loss: 106.430084
</span></span><span class="line"><span class="cl">Train Epoch: 8 [38400/60000 (64%)]  Loss: 106.380074
</span></span><span class="line"><span class="cl">Train Epoch: 8 [51200/60000 (85%)]  Loss: 103.021561
</span></span><span class="line"><span class="cl">====&gt; Epoch: 8 Average loss: 107.3205
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 106.6568
</span></span><span class="line"><span class="cl">Train Epoch: 9 [0/60000 (0%)]   Loss: 106.435928
</span></span><span class="line"><span class="cl">Train Epoch: 9 [12800/60000 (21%)]  Loss: 105.544891
</span></span><span class="line"><span class="cl">Train Epoch: 9 [25600/60000 (43%)]  Loss: 102.952591
</span></span><span class="line"><span class="cl">Train Epoch: 9 [38400/60000 (64%)]  Loss: 103.070465
</span></span><span class="line"><span class="cl">Train Epoch: 9 [51200/60000 (85%)]  Loss: 105.689209
</span></span><span class="line"><span class="cl">====&gt; Epoch: 9 Average loss: 106.7969
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 106.0421
</span></span><span class="line"><span class="cl">Train Epoch: 10 [0/60000 (0%)]  Loss: 106.396545
</span></span><span class="line"><span class="cl">Train Epoch: 10 [12800/60000 (21%)] Loss: 105.038795
</span></span><span class="line"><span class="cl">Train Epoch: 10 [25600/60000 (43%)] Loss: 105.274765
</span></span><span class="line"><span class="cl">Train Epoch: 10 [38400/60000 (64%)] Loss: 104.411789
</span></span><span class="line"><span class="cl">Train Epoch: 10 [51200/60000 (85%)] Loss: 104.329590
</span></span><span class="line"><span class="cl">====&gt; Epoch: 10 Average loss: 106.3689
</span></span><span class="line"><span class="cl">====&gt; Test set loss: 105.5585</span></span></code></pre></div></div>
<div class="details admonition note">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw" aria-hidden="true"></i>Bibliography<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><h2 id="bibliography">Bibliography</h2>
<ul>
<li>Doersch, Carl. 2021. Tutorial on Variational Autoencoders. January 3, 2021. <a href="http://arxiv.org/abs/1606.05908" target="_blank" rel="noopener noreffer ">http://arxiv.org/abs/1606.05908</a>.</li>
<li>Kingma, Diederik P., and Max Welling. 2019. An Introduction to Variational Autoencoders. Foundations and Trends in Machine Learning 12 (4): 30792. <a href="https://doi.org/10.1561/2200000056" target="_blank" rel="noopener noreffer ">https://doi.org/10.1561/2200000056</a>.</li>
<li>Ramchandran, Siddharth, Gleb Tikhonov, Otto Lnnroth, Pekka Tiikkainen, and Harri Lhdesmki. 2022. Learning Conditional Variational Autoencoders with Missing Covariates. March 2, 2022. <a href="http://arxiv.org/abs/2203.01218" target="_blank" rel="noopener noreffer ">http://arxiv.org/abs/2203.01218</a>.</li>
<li>Yunfan Jiang, ELBO  What &amp; Why,Jan 11, 2021, in <a href="https://yunfanj.com/blog/2021/01/11/ELBO.html" target="_blank" rel="noopener noreffer ">https://yunfanj.com/blog/2021/01/11/ELBO.html</a>.</li>
</ul>
</div>
        </div>
    </div>
<p>Pic by <a href="https://www.freepik.es/foto-gratis/laboratorio-computacion-brillante-equipo-moderno-tecnologia-generada-ia_41451597.htm">@vecstock</a>, <a href="https://www.freepik.es/">Freepik</a></p></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-09-01</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/variational_autoencoders/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on X" data-sharer="x" data-url="http://localhost:1313/variational_autoencoders/" data-title="Generative Models: Variational Autoencoders" data-via="clausecastillo" data-hashtags="expectation maximization,variational autoencoders,deep neural networks"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Threads" data-sharer="threads" data-url="http://localhost:1313/variational_autoencoders/" data-title="Generative Models: Variational Autoencoders"><i class="fab fa-threads fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="http://localhost:1313/variational_autoencoders/" data-hashtag="expectation maximization"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="http://localhost:1313/variational_autoencoders/" data-title="Generative Models: Variational Autoencoders"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="http://localhost:1313/variational_autoencoders/" data-title="Generative Models: Variational Autoencoders"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on " data-sharer="weibo" data-url="http://localhost:1313/variational_autoencoders/" data-title="Generative Models: Variational Autoencoders"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Diaspora" data-sharer="diaspora" data-url="http://localhost:1313/variational_autoencoders/" data-title="Generative Models: Variational Autoencoders" data-description=""><i class="fab fa-diaspora fa-fw" aria-hidden="true"></i></a><a href="https://t.me/share/url?url=http%3a%2f%2flocalhost%3a1313%2fvariational_autoencoders%2f&amp;text=Generative%20Models%3a%20Variational%20Autoencoders" target="_blank" title="Share on Telegram"><i class="fab fa-telegram fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/expectation-maximization/">Expectation Maximization</a>,&nbsp;<a href="/tags/variational-autoencoders/">Variational Autoencoders</a>,&nbsp;<a href="/tags/deep-neural-networks/">Deep Neural Networks</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/documents_qa/" class="prev" rel="prev" title="Retrieval-augmented generation for Chatbot"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Retrieval-augmented generation for Chatbot</a>
            <a href="/genetic_algorithms/" class="next" rel="next" title="Can we imitate Nature&#39;s evolutionary abilities? ">Can we imitate Nature's evolutionary abilities? <i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.3.1-DEV"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2023 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">xxxx</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/lightgallery/css/lightgallery-bundle.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js"></script><script src="/lib/algoliasearch/lite/browser.umd.js"></script><script src="/lib/lazysizes/lazysizes.min.js"></script><script src="/lib/lightgallery/lightgallery.min.js"></script><script src="/lib/lightgallery/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="/lib/lightgallery/plugins/zoom/lg-zoom.min.js"></script><script src="/lib/clipboard/clipboard.min.js"></script><script src="/lib/sharer/sharer.min.js"></script><script src="/lib/katex/katex.min.js"></script><script src="/lib/katex/contrib/auto-render.min.js"></script><script src="/lib/katex/contrib/copy-tex.min.js"></script><script src="/lib/katex/contrib/mhchem.min.js"></script><script src="/lib/cookieconsent/cookieconsent.min.js"></script><script>window.config={"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"lightgallery":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.en","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script src="/js/theme.min.js"></script></body>
</html>
